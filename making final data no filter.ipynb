{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5728d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1470d872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['16_with_adress.csv', '6_with_adress.csv', '51_with_adress.csv', '48_with_adress.csv', '32_with_adress.csv', '47_with_adress.csv', '9_with_adress.csv', '19_with_adress.csv', '24_with_adress.csv', '59_with_adress.csv', '23_with_adress.csv', '40_with_adress.csv', '35_with_adress.csv', '56_with_adress.csv', '1_with_adress.csv', '11_with_adress.csv', '22_with_adress.csv', '58_with_adress.csv', '41_with_adress.csv', '34_with_adress.csv', '10_with_adress.csv', '57_with_adress.csv', '50_with_adress.csv', '17_with_adress.csv', '7_with_adress.csv', '33_with_adress.csv', '49_with_adress.csv', '46_with_adress.csv', '8_with_adress.csv', '18_with_adress.csv', '25_with_adress.csv', '43_with_adress.csv', '39_with_adress.csv', '20_with_adress.csv', '12_with_adress.csv', '2_with_adress.csv', '55_with_adress.csv', '36_with_adress.csv', '31_with_adress.csv', '52_with_adress.csv', '5_with_adress.csv', '15_with_adress.csv', '28_with_adress.csv', '60_with_adress.csv', '27_with_adress.csv', 'data_no_filter.zip', '44_with_adress.csv', '30_with_adress.csv', '4_with_adress.csv', '14_with_adress.csv', '29_with_adress.csv', '53_with_adress.csv', '26_with_adress.csv', '45_with_adress.csv', '38_with_adress.csv', '42_with_adress.csv', '21_with_adress.csv', '54_with_adress.csv', '13_with_adress.csv', '3_with_adress.csv', '37_with_adress.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('./map no filter'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a52943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qq/ckqvfttx4vs_68cbq08jts2c0000gn/T/ipykernel_53551/3877665576.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df=pd.concat([df,data])\n",
      "/var/folders/qq/ckqvfttx4vs_68cbq08jts2c0000gn/T/ipykernel_53551/3877665576.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df=pd.concat([df,data])\n",
      "/var/folders/qq/ckqvfttx4vs_68cbq08jts2c0000gn/T/ipykernel_53551/3877665576.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df=pd.concat([df,data])\n",
      "/var/folders/qq/ckqvfttx4vs_68cbq08jts2c0000gn/T/ipykernel_53551/3877665576.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df=pd.concat([df,data])\n",
      "/var/folders/qq/ckqvfttx4vs_68cbq08jts2c0000gn/T/ipykernel_53551/3877665576.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df=pd.concat([df,data])\n",
      "/var/folders/qq/ckqvfttx4vs_68cbq08jts2c0000gn/T/ipykernel_53551/3877665576.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df=pd.concat([df,data])\n",
      "/var/folders/qq/ckqvfttx4vs_68cbq08jts2c0000gn/T/ipykernel_53551/3877665576.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df=pd.concat([df,data])\n",
      "/var/folders/qq/ckqvfttx4vs_68cbq08jts2c0000gn/T/ipykernel_53551/3877665576.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df=pd.concat([df,data])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Multiple files found in ZIP file. Only one file per ZIP: ['1_with_adress.csv', '__MACOSX/._1_with_adress.csv', '2_with_adress.csv', '3_with_adress.csv', '4_with_adress.csv', '5_with_adress.csv', '6_with_adress.csv', '7_with_adress.csv', '8_with_adress.csv', '9_with_adress.csv', '10_with_adress.csv', '11_with_adress.csv', '12_with_adress.csv', '13_with_adress.csv', '14_with_adress.csv', '15_with_adress.csv', '16_with_adress.csv', '17_with_adress.csv', '18_with_adress.csv', '19_with_adress.csv', '20_with_adress.csv', '21_with_adress.csv', '22_with_adress.csv', '23_with_adress.csv', '24_with_adress.csv', '25_with_adress.csv', '26_with_adress.csv', '27_with_adress.csv', '28_with_adress.csv', '29_with_adress.csv', '30_with_adress.csv', '31_with_adress.csv', '32_with_adress.csv', '33_with_adress.csv', '34_with_adress.csv', '35_with_adress.csv', '36_with_adress.csv', '37_with_adress.csv', '38_with_adress.csv', '39_with_adress.csv', '40_with_adress.csv', '41_with_adress.csv', '42_with_adress.csv', '43_with_adress.csv', '44_with_adress.csv', '45_with_adress.csv', '46_with_adress.csv', '47_with_adress.csv', '48_with_adress.csv', '49_with_adress.csv', '50_with_adress.csv', '51_with_adress.csv', '52_with_adress.csv', '53_with_adress.csv', '54_with_adress.csv', '55_with_adress.csv', '56_with_adress.csv', '57_with_adress.csv', '58_with_adress.csv', '59_with_adress.csv']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./map no filter\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./map no filter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/common.py:795\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZero files found in ZIP file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_buf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 795\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    796\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiple files found in ZIP file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one file per ZIP: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    798\u001b[0m             )\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# TAR Encoding\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compression \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtar\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Multiple files found in ZIP file. Only one file per ZIP: ['1_with_adress.csv', '__MACOSX/._1_with_adress.csv', '2_with_adress.csv', '3_with_adress.csv', '4_with_adress.csv', '5_with_adress.csv', '6_with_adress.csv', '7_with_adress.csv', '8_with_adress.csv', '9_with_adress.csv', '10_with_adress.csv', '11_with_adress.csv', '12_with_adress.csv', '13_with_adress.csv', '14_with_adress.csv', '15_with_adress.csv', '16_with_adress.csv', '17_with_adress.csv', '18_with_adress.csv', '19_with_adress.csv', '20_with_adress.csv', '21_with_adress.csv', '22_with_adress.csv', '23_with_adress.csv', '24_with_adress.csv', '25_with_adress.csv', '26_with_adress.csv', '27_with_adress.csv', '28_with_adress.csv', '29_with_adress.csv', '30_with_adress.csv', '31_with_adress.csv', '32_with_adress.csv', '33_with_adress.csv', '34_with_adress.csv', '35_with_adress.csv', '36_with_adress.csv', '37_with_adress.csv', '38_with_adress.csv', '39_with_adress.csv', '40_with_adress.csv', '41_with_adress.csv', '42_with_adress.csv', '43_with_adress.csv', '44_with_adress.csv', '45_with_adress.csv', '46_with_adress.csv', '47_with_adress.csv', '48_with_adress.csv', '49_with_adress.csv', '50_with_adress.csv', '51_with_adress.csv', '52_with_adress.csv', '53_with_adress.csv', '54_with_adress.csv', '55_with_adress.csv', '56_with_adress.csv', '57_with_adress.csv', '58_with_adress.csv', '59_with_adress.csv']"
     ]
    }
   ],
   "source": [
    "df=None\n",
    "for file_name in os.listdir('./map no filter'):\n",
    "     if not file_name.endswith('csv'):\n",
    "        continue\n",
    "    data=pd.read_csv(os.path.join('./map no filter',file_name))\n",
    "    data=data.drop('Unnamed: 0',axis=1)\n",
    "    if df is None:\n",
    "        df=data\n",
    "    else:\n",
    "        df=pd.concat([df,data])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a06dd57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.hall.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6930635",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['type']=='vending_machine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_data(type_at,hall):\n",
    "\n",
    "    if type_at=='university':\n",
    "        if hall=='Dartmouth College':\n",
    "            return 'home_time'\n",
    "        else: return 'school_time'\n",
    "    elif type_at in ['dormitory', 'house','apartments','residential']:\n",
    "        return 'home_time'\n",
    "    elif type_at in ['college','school','educational_institution']:\n",
    "        return 'school_time'\n",
    "    elif type_at in ['bicycle_parking','parking','bus_stop','pedestrian','primary','cycleway', 'railway_station','car_repair']:\n",
    "        return 'travel_time'\n",
    "    elif type_at in ['beauty','electronics', 'wholesale','interior_decoration','service','retail','florist','bakery']:\n",
    "        return 'shopping'\n",
    "    elif type_at in ['industrial','veterinary','office', 'social_centre','social_facility']:\n",
    "        return 'working'\n",
    "    elif type_at in['books','baking','pub','convenience', 'public_bookcase','golf_course','cafe','fast_food','books','bar','clothes', 'pottery', 'hotel','outdoor_seating', 'museum','pitch','sports_centre','library', 'arts_centre', 'grandstand', 'restaurant', 'bench', 'courtyard']:\n",
    "        return 'recreational_activities'\n",
    "    else: return 'orthers'\n",
    "        \n",
    "df['new_type']=df.apply(lambda df: group_data(df['type'],df['hall']),axis=1)\n",
    "df['time_difference']=list(df['time_difference'][1:])+[np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1053ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.fillna(0)\n",
    "df.new_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b94299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_check=df[df['tnv']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82463746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "def convert_day(time):\n",
    "\n",
    "    time_conv=datetime.strptime(time,'%d:%m:%Y')\n",
    "\n",
    "    time_re=time_conv.strftime(\"%w\")\n",
    "    return time_re\n",
    "def trim(x):\n",
    "    try:\n",
    "        return x.strip()\n",
    "    except: \n",
    "        print(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee6c71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # change convertdate to '... ' to work this function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data_1=data_check.groupby(['date','new_type'])['time_difference'].sum().to_frame()\n",
    "# data_1=data_1.reset_index()\n",
    "# data_vol_1=None\n",
    "# for date in data_1.date.unique():\n",
    "#     data_show=data_1[data_1['date']==date].set_index(\"new_type\").iloc[:,1].T\n",
    "#     if data_vol_1 is None:\n",
    "#         data_vol_1=data_show\n",
    "#     else:\n",
    "#         data_vol_1=pd.concat([data_vol_1,data_show],axis=1)\n",
    "# data_vol_1=data_vol_1.T\n",
    "# data_vol_1['date']=data_1.date.unique()\n",
    "# data_vol_1['date']=data_vol_1['date']\n",
    "# print(type(data_vol_1['date'][0]))\n",
    "# ls=[]\n",
    "# for date in data_vol_1['date']:\n",
    "#     ls.append(convert_day(date))\n",
    "# data_vol_1['date_in_week']=ls\n",
    "# data_vol_1=data_vol_1.set_index('date')\n",
    "# data_vol_1['volunteer']=[1]*data_vol_1.shape[0]\n",
    "# data_vol_1=data_vol_1.fillna(0)                                   \n",
    "# data_vol_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_data=pd.read_csv('./time_learn_in_day.csv')\n",
    "class_data=class_data.set_index('tnv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf64f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tnv in df.tnv.unique():\n",
    "    location_names=None\n",
    "    data_check=df[df['tnv']==tnv]\n",
    "    data_check['date']=data_check['date'].apply(lambda x:trim(x))\n",
    "    data_check['hall']=data_check['hall'].astype(str)\n",
    "    data_check['location_names']=data_check.groupby(['date'])['hall'].transform(','.join)\n",
    "    location_names=data_check[['date','location_names']].drop_duplicates()\n",
    "        \n",
    "    location_names['date']=location_names['date'].apply(lambda x:trim(x))\n",
    "    location_names=location_names.set_index('date')\n",
    "    \n",
    "    data_tnv=data_check.groupby(['date','new_type'])['time_difference'].sum().to_frame()\n",
    "    data_tnv=data_tnv.reset_index()\n",
    "    data_vol_tnv=None\n",
    "    for date in data_tnv.date.unique():\n",
    "        data_show=data_tnv[data_tnv['date']==date].set_index(\"new_type\").iloc[:,1].T\n",
    "        if data_vol_tnv is None:\n",
    "            data_vol_tnv=data_show\n",
    "        else:\n",
    "            data_vol_tnv=pd.concat([data_vol_tnv,data_show],axis=1)\n",
    "    data_vol_tnv=data_vol_tnv.T\n",
    "    data_vol_tnv['date']=data_tnv.date.unique()\n",
    "    data_vol_tnv['date']=data_vol_tnv['date']\n",
    "    ls=[]\n",
    "    for date in data_vol_tnv['date']:\n",
    "        ls.append(int(convert_day(date))-1)\n",
    "    data_vol_tnv['date_in_week']=ls\n",
    "    data_vol_tnv=data_vol_tnv.set_index('date')\n",
    "    data_vol_tnv['volunteer']=[tnv]*data_vol_tnv.shape[0]\n",
    "    \n",
    "    data_vol_tnv=pd.concat([data_vol_tnv,location_names],axis=1)\n",
    "    data_vol_tnv=data_vol_tnv.fillna(0)                                   \n",
    "    data_vol_tnv.to_csv(f'./final_data_no_cut_class_no_filter/{tnv}_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17912996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cut_class(time,date,tnv):\n",
    "    if date>-1:\n",
    "        try:\n",
    "            learning_schedule=eval(class_data.loc[tnv,'time_learn_day'])\n",
    "\n",
    "            if time < 0.7*float(learning_schedule[date]):\n",
    "                return 1\n",
    "            return 0\n",
    "        except: pass\n",
    "#     print('error')\n",
    "    return np.nan\n",
    "def time_to_learn(date,tnv):\n",
    "    if date>-1:\n",
    "        try:\n",
    "            learning_schedule=eval(class_data.loc[tnv,'time_learn_day'])\n",
    "\n",
    "            return (learning_schedule[date])\n",
    "        except: pass\n",
    "    print('error')\n",
    "    return np.nan\n",
    "a=is_cut_class(12,1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b546536",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tnv in df.tnv.unique():\n",
    "    try:\n",
    "        print(tnv)\n",
    "        location_names=None\n",
    "        data_check=df[df['tnv']==tnv]\n",
    "        data_check['date']=data_check['date'].apply(lambda x:trim(x))\n",
    "        data_check['hall']=data_check['hall'].astype(str)\n",
    "        data_check['location_names']=data_check.groupby(['date'])['hall'].transform(','.join)\n",
    "        location_names=data_check[['date','location_names']].drop_duplicates()\n",
    "        \n",
    "        location_names['date']=location_names['date'].apply(lambda x:trim(x))\n",
    "        location_names=location_names.set_index('date')\n",
    "    \n",
    "        data_tnv=data_check.groupby(['date','new_type'])['time_difference'].sum().to_frame()\n",
    "        data_tnv=data_tnv.reset_index()\n",
    "        data_vol_tnv=None\n",
    "        for date in data_tnv.date.unique():\n",
    "            data_show=data_tnv[data_tnv['date']==date].set_index(\"new_type\").iloc[:,1].T\n",
    "            if data_vol_tnv is None:\n",
    "                data_vol_tnv=data_show\n",
    "            else:\n",
    "                data_vol_tnv=pd.concat([data_vol_tnv,data_show],axis=1)\n",
    "        data_vol_tnv=data_vol_tnv.T\n",
    "        data_vol_tnv['date']=data_tnv.date.unique()\n",
    "        data_vol_tnv['date']=data_vol_tnv['date']\n",
    "        ls=[]\n",
    "        for date in data_vol_tnv['date']:\n",
    "            ls.append(int(convert_day(date))-1)\n",
    "        data_vol_tnv['date_in_week']=ls\n",
    "        data_vol_tnv=data_vol_tnv.fillna(0) \n",
    "        data_vol_tnv['cut_class']=data_vol_tnv.apply(lambda df: is_cut_class(df['school_time'],\n",
    "    df['date_in_week'],tnv),axis=1)\n",
    "        data_vol_tnv['class_schedule']=data_vol_tnv.apply(lambda df: time_to_learn(\n",
    "    df['date_in_week'],tnv),axis=1)\n",
    "        data_vol_tnv=data_vol_tnv.set_index('date')\n",
    "        data_vol_tnv['volunteer']=[tnv]*data_vol_tnv.shape[0]\n",
    "        data_check['location_names']=data_check.groupby(['date'])['hall'].transform(','.join)\n",
    "        location_names=data_check[['date','location_names']].drop_duplicates().set_index('date')\n",
    "        data_vol_tnv=pd.concat([data_vol_tnv,location_names],axis=1)\n",
    "        data_vol_tnv=data_vol_tnv.fillna(0) \n",
    "        data_vol_tnv.to_csv(f'./final_data_no_filter/{tnv}_data.csv')\n",
    "    except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b27c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vol_tnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d25c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd85eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26cae8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cac571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05d5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
